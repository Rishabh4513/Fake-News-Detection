{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Nov  5 12:58:52 2017\n",
        "\n",
        "@author: NishitP\n",
        "\"\"\"\n",
        "\n",
        "import DataPrep\n",
        "import FeatureSelection\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "#string to test\n",
        "doc_new = ['obama is running for president in 2016']\n",
        "\n",
        "#the feature selection has been done in FeatureSelection.py module. here we will create models using those features for prediction\n",
        "\n",
        "#first we will use bag of words techniques\n",
        "\n",
        "#building classifier using naive bayes \n",
        "nb_pipeline = Pipeline([\n",
        "        ('NBCV',FeatureSelection.countV),\n",
        "        ('nb_clf',MultinomialNB())])\n",
        "\n",
        "nb_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_nb = nb_pipeline.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_nb == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#building classifier using logistic regression\n",
        "logR_pipeline = Pipeline([\n",
        "        ('LogRCV',FeatureSelection.countV),\n",
        "        ('LogR_clf',LogisticRegression())\n",
        "        ])\n",
        "\n",
        "logR_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_LogR = logR_pipeline.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_LogR == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#building Linear SVM classfier\n",
        "svm_pipeline = Pipeline([\n",
        "        ('svmCV',FeatureSelection.countV),\n",
        "        ('svm_clf',svm.LinearSVC())\n",
        "        ])\n",
        "\n",
        "svm_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_svm = svm_pipeline.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_svm == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#using SVM Stochastic Gradient Descent on hinge loss\n",
        "sgd_pipeline = Pipeline([\n",
        "        ('svm2CV',FeatureSelection.countV),\n",
        "        ('svm2_clf',SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5))\n",
        "        ])\n",
        "\n",
        "sgd_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_sgd = sgd_pipeline.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_sgd == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#random forest\n",
        "random_forest = Pipeline([\n",
        "        ('rfCV',FeatureSelection.countV),\n",
        "        ('rf_clf',RandomForestClassifier(n_estimators=200,n_jobs=3))\n",
        "        ])\n",
        "    \n",
        "random_forest.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_rf = random_forest.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_rf == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#User defined functon for K-Fold cross validatoin\n",
        "def build_confusion_matrix(classifier):\n",
        "    \n",
        "    k_fold = KFold(n_splits=5)\n",
        "    scores = []\n",
        "    confusion = np.array([[0,0],[0,0]])\n",
        "\n",
        "    for train_ind, test_ind in k_fold.split(DataPrep.train_news):\n",
        "        train_text = DataPrep.train_news.iloc[train_ind]['Statement'] \n",
        "        train_y = DataPrep.train_news.iloc[train_ind]['Label']\n",
        "    \n",
        "        test_text = DataPrep.train_news.iloc[test_ind]['Statement']\n",
        "        test_y = DataPrep.train_news.iloc[test_ind]['Label']\n",
        "        \n",
        "        classifier.fit(train_text,train_y)\n",
        "        predictions = classifier.predict(test_text)\n",
        "        \n",
        "        confusion += confusion_matrix(test_y,predictions)\n",
        "        score = f1_score(test_y,predictions)\n",
        "        scores.append(score)\n",
        "    \n",
        "    return (print('Total statements classified:', len(DataPrep.train_news)),\n",
        "    print('Score:', sum(scores)/len(scores)),\n",
        "    print('score length', len(scores)),\n",
        "    print('Confusion matrix:'),\n",
        "    print(confusion))\n",
        "    \n",
        "#K-fold cross validation for all classifiers\n",
        "build_confusion_matrix(nb_pipeline)\n",
        "build_confusion_matrix(logR_pipeline)\n",
        "build_confusion_matrix(svm_pipeline)\n",
        "build_confusion_matrix(sgd_pipeline)\n",
        "build_confusion_matrix(random_forest)\n",
        "\n",
        "#========================================================================================\n",
        "#Bag of words confusion matrix and F1 scores\n",
        "\n",
        "#Naive bayes\n",
        "# [2118 2370]\n",
        "# [1664 4088]\n",
        "# f1-Score: 0.669611539651\n",
        "\n",
        "#Logistic regression\n",
        "# [2252 2236]\n",
        "# [1933 3819]\n",
        "# f1-Score: 0.646909097798\n",
        "\n",
        "#svm\n",
        "# [2260 2228]\n",
        "# [2246 3506]\n",
        "#f1-score: 0.610468748792\n",
        "\n",
        "#sgdclassifier\n",
        "# [2414 2074]\n",
        "# [2042 3710]\n",
        "# f1-Score: 0.640874558778\n",
        "\n",
        "#random forest classifier\n",
        "# [1821 2667]\n",
        "# [1192 4560]\n",
        "# f1-Score: 0.702651511011\n",
        "#=========================================================================================\n",
        "\n",
        "\n",
        "\"\"\"So far we have used bag of words technique to extract the features and passed those featuers into classifiers. We have also seen the\n",
        "f1 scores of these classifiers. now lets enhance these features using term frequency weights with various n-grams\n",
        "\"\"\"\n",
        "\n",
        "##Now using n-grams\n",
        "#naive-bayes classifier\n",
        "nb_pipeline_ngram = Pipeline([\n",
        "        ('nb_tfidf',FeatureSelection.tfidf_ngram),\n",
        "        ('nb_clf',MultinomialNB())])\n",
        "\n",
        "nb_pipeline_ngram.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_nb_ngram = nb_pipeline_ngram.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_nb_ngram == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#logistic regression classifier\n",
        "logR_pipeline_ngram = Pipeline([\n",
        "        ('LogR_tfidf',FeatureSelection.tfidf_ngram),\n",
        "        ('LogR_clf',LogisticRegression(penalty=\"l2\",C=1))\n",
        "        ])\n",
        "\n",
        "logR_pipeline_ngram.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_LogR_ngram = logR_pipeline_ngram.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_LogR_ngram == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#linear SVM classifier\n",
        "svm_pipeline_ngram = Pipeline([\n",
        "        ('svm_tfidf',FeatureSelection.tfidf_ngram),\n",
        "        ('svm_clf',svm.LinearSVC())\n",
        "        ])\n",
        "\n",
        "svm_pipeline_ngram.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_svm_ngram = svm_pipeline_ngram.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_svm_ngram == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#sgd classifier\n",
        "sgd_pipeline_ngram = Pipeline([\n",
        "         ('sgd_tfidf',FeatureSelection.tfidf_ngram),\n",
        "         ('sgd_clf',SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5))\n",
        "         ])\n",
        "\n",
        "sgd_pipeline_ngram.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_sgd_ngram = sgd_pipeline_ngram.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_sgd_ngram == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#random forest classifier\n",
        "random_forest_ngram = Pipeline([\n",
        "        ('rf_tfidf',FeatureSelection.tfidf_ngram),\n",
        "        ('rf_clf',RandomForestClassifier(n_estimators=300,n_jobs=3))\n",
        "        ])\n",
        "    \n",
        "random_forest_ngram.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_rf_ngram = random_forest_ngram.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_rf_ngram == DataPrep.test_news['Label'])\n",
        "\n",
        "\n",
        "#K-fold cross validation for all classifiers\n",
        "build_confusion_matrix(nb_pipeline_ngram)\n",
        "build_confusion_matrix(logR_pipeline_ngram)\n",
        "build_confusion_matrix(svm_pipeline_ngram)\n",
        "build_confusion_matrix(sgd_pipeline_ngram)\n",
        "build_confusion_matrix(random_forest_ngram)\n",
        "\n",
        "#========================================================================================\n",
        "#n-grams & tfidf confusion matrix and F1 scores\n",
        "\n",
        "#Naive bayes\n",
        "# [841 3647]\n",
        "# [427 5325]\n",
        "# f1-Score: 0.723262051071\n",
        "\n",
        "#Logistic regression\n",
        "# [1617 2871]\n",
        "# [1097 4655]\n",
        "# f1-Score: 0.70113000531\n",
        "\n",
        "#svm\n",
        "# [2016 2472]\n",
        "# [1524 4228]\n",
        "# f1-Score: 0.67909201429\n",
        "\n",
        "#sgdclassifier\n",
        "# [  10 4478]\n",
        "# [  13 5739]\n",
        "# f1-Score: 0.718731637053\n",
        "\n",
        "#random forest\n",
        "# [1979 2509]\n",
        "# [1630 4122]\n",
        "# f1-Score: 0.665720333284\n",
        "#=========================================================================================\n",
        "\n",
        "print(classification_report(DataPrep.test_news['Label'], predicted_nb_ngram))\n",
        "print(classification_report(DataPrep.test_news['Label'], predicted_LogR_ngram))\n",
        "print(classification_report(DataPrep.test_news['Label'], predicted_svm_ngram))\n",
        "print(classification_report(DataPrep.test_news['Label'], predicted_sgd_ngram))\n",
        "print(classification_report(DataPrep.test_news['Label'], predicted_rf_ngram))\n",
        "\n",
        "DataPrep.test_news['Label'].shape\n",
        "\n",
        "\"\"\"\n",
        "Out of all the models fitted, we would take 2 best performing model. we would call them candidate models\n",
        "from the confusion matrix, we can see that random forest and logistic regression are best performing \n",
        "in terms of precision and recall (take a look into false positive and true negative counts which appeares\n",
        "to be low compared to rest of the models)\n",
        "\"\"\"\n",
        "\n",
        "#grid-search parameter optimization\n",
        "#random forest classifier parameters\n",
        "parameters = {'rf_tfidf__ngram_range': [(1, 1), (1, 2),(1,3),(1,4),(1,5)],\n",
        "               'rf_tfidf__use_idf': (True, False),\n",
        "               'rf_clf__max_depth': (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15)\n",
        "}\n",
        "\n",
        "gs_clf = GridSearchCV(random_forest_ngram, parameters, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(DataPrep.train_news['Statement'][:10000],DataPrep.train_news['Label'][:10000])\n",
        "\n",
        "gs_clf.best_score_\n",
        "gs_clf.best_params_\n",
        "gs_clf.cv_results_\n",
        "\n",
        "#logistic regression parameters\n",
        "parameters = {'LogR_tfidf__ngram_range': [(1, 1), (1, 2),(1,3),(1,4),(1,5)],\n",
        "               'LogR_tfidf__use_idf': (True, False),\n",
        "               'LogR_tfidf__smooth_idf': (True, False)\n",
        "}\n",
        "\n",
        "gs_clf = GridSearchCV(logR_pipeline_ngram, parameters, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(DataPrep.train_news['Statement'][:10000],DataPrep.train_news['Label'][:10000])\n",
        "\n",
        "gs_clf.best_score_\n",
        "gs_clf.best_params_\n",
        "gs_clf.cv_results_\n",
        "\n",
        "#Linear SVM \n",
        "parameters = {'svm_tfidf__ngram_range': [(1, 1), (1, 2),(1,3),(1,4),(1,5)],\n",
        "               'svm_tfidf__use_idf': (True, False),\n",
        "               'svm_tfidf__smooth_idf': (True, False),\n",
        "               'svm_clf__penalty': ('l1','l2'),\n",
        "}\n",
        "\n",
        "gs_clf = GridSearchCV(svm_pipeline_ngram, parameters, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(DataPrep.train_news['Statement'][:10000],DataPrep.train_news['Label'][:10000])\n",
        "\n",
        "gs_clf.best_score_\n",
        "gs_clf.best_params_\n",
        "gs_clf.cv_results_\n",
        "\n",
        "#by running above commands we can find the model with best performing parameters\n",
        "\n",
        "\n",
        "#running both random forest and logistic regression models again with best parameter found with GridSearch method\n",
        "random_forest_final = Pipeline([\n",
        "        ('rf_tfidf',TfidfVectorizer(stop_words='english',ngram_range=(1,3),use_idf=True,smooth_idf=True)),\n",
        "        ('rf_clf',RandomForestClassifier(n_estimators=300,n_jobs=3,max_depth=10))\n",
        "        ])\n",
        "    \n",
        "random_forest_final.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_rf_final = random_forest_final.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_rf_final == DataPrep.test_news['Label'])\n",
        "print(metrics.classification_report(DataPrep.test_news['Label'], predicted_rf_final))\n",
        "\n",
        "logR_pipeline_final = Pipeline([\n",
        "        #('LogRCV',countV_ngram),\n",
        "        ('LogR_tfidf',TfidfVectorizer(stop_words='english',ngram_range=(1,5),use_idf=True,smooth_idf=False)),\n",
        "        ('LogR_clf',LogisticRegression(penalty=\"l2\",C=1))\n",
        "        ])\n",
        "\n",
        "logR_pipeline_final.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
        "predicted_LogR_final = logR_pipeline_final.predict(DataPrep.test_news['Statement'])\n",
        "np.mean(predicted_LogR_final == DataPrep.test_news['Label'])\n",
        "#accuracy = 0.62\n",
        "print(metrics.classification_report(DataPrep.test_news['Label'], predicted_LogR_final))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "by running both random forest and logistic regression with GridSearch's best parameter estimation, we found that for random \n",
        "forest model with n-gram has better accuracty than with the parameter estimated. The logistic regression model with best parameter \n",
        "has almost similar performance as n-gram model so logistic regression will be out choice of model for prediction.\n",
        "\"\"\"\n",
        "\n",
        "#saving best model to the disk\n",
        "model_file = 'final_model.sav'\n",
        "pickle.dump(logR_pipeline_ngram,open(model_file,'wb'))\n",
        "\n",
        "\n",
        "#Plotting learing curve\n",
        "def plot_learing_curve(pipeline,title):\n",
        "    size = 10000\n",
        "    cv = KFold(size, shuffle=True)\n",
        "    \n",
        "    X = DataPrep.train_news[\"Statement\"]\n",
        "    y = DataPrep.train_news[\"Label\"]\n",
        "    \n",
        "    pl = pipeline\n",
        "    pl.fit(X,y)\n",
        "    \n",
        "    train_sizes, train_scores, test_scores = learning_curve(pl, X, y, n_jobs=-1, cv=cv, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
        "       \n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "     \n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # box-like grid\n",
        "    plt.grid()\n",
        "    \n",
        "    # plot the std deviation as a transparent range at each training set size\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    \n",
        "    # plot the average training and test score lines at each training set size\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "    \n",
        "    # sizes the window for readability and displays the plot\n",
        "    # shows error from 0 to 1.1\n",
        "    plt.ylim(-.1,1.1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#below command will plot learing curves for each of the classifiers\n",
        "plot_learing_curve(logR_pipeline_ngram,\"Naive-bayes Classifier\")\n",
        "plot_learing_curve(nb_pipeline_ngram,\"LogisticRegression Classifier\")\n",
        "plot_learing_curve(svm_pipeline_ngram,\"SVM Classifier\")\n",
        "plot_learing_curve(sgd_pipeline_ngram,\"SGD Classifier\")\n",
        "plot_learing_curve(random_forest_ngram,\"RandomForest Classifier\")\n",
        "\n",
        "\"\"\"\n",
        "by plotting the learning cureve for logistic regression, it can be seen that cross-validation score is stagnating throughout and it \n",
        "is unable to learn from data. Also we see that there are high errors that indicates model is simple and we may want to increase the\n",
        "model complexity.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#plotting Precision-Recall curve\n",
        "def plot_PR_curve(classifier):\n",
        "    \n",
        "    precision, recall, thresholds = precision_recall_curve(DataPrep.test_news['Label'], classifier)\n",
        "    average_precision = average_precision_score(DataPrep.test_news['Label'], classifier)\n",
        "    \n",
        "    plt.step(recall, precision, color='b', alpha=0.2,\n",
        "             where='post')\n",
        "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                     color='b')\n",
        "    \n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.title('2-class Random Forest Precision-Recall curve: AP={0:0.2f}'.format(\n",
        "              average_precision))\n",
        "    \n",
        "plot_PR_curve(predicted_LogR_ngram)\n",
        "plot_PR_curve(predicted_rf_ngram)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Now let's extract the most informative feature from ifidf vectorizer for all fo the classifiers and see of there are any common\n",
        "words that we can identify i.e. are these most informative feature acorss the classifiers are same? we will create a function that \n",
        "will extract top 50 features.\n",
        "\"\"\"\n",
        "\n",
        "def show_most_informative_features(model, vect, clf, text=None, n=50):\n",
        "    # Extract the vectorizer and the classifier from the pipeline\n",
        "    vectorizer = model.named_steps[vect]\n",
        "    classifier = model.named_steps[clf]\n",
        "\n",
        "     # Check to make sure that we can perform this computation\n",
        "    if not hasattr(classifier, 'coef_'):\n",
        "        raise TypeError(\n",
        "            \"Cannot compute most informative features on {}.\".format(\n",
        "                classifier.__class__.__name__\n",
        "            )\n",
        "        )\n",
        "            \n",
        "    if text is not None:\n",
        "        # Compute the coefficients for the text\n",
        "        tvec = model.transform([text]).toarray()\n",
        "    else:\n",
        "        # Otherwise simply use the coefficients\n",
        "        tvec = classifier.coef_\n",
        "\n",
        "    # Zip the feature names with the coefs and sort\n",
        "    coefs = sorted(\n",
        "        zip(tvec[0], vectorizer.get_feature_names()),\n",
        "        reverse=True\n",
        "    )\n",
        "    \n",
        "    # Get the top n and bottom n coef, name pairs\n",
        "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
        "\n",
        "    # Create the output string to return\n",
        "    output = []\n",
        "\n",
        "    # If text, add the predicted value to the output.\n",
        "    if text is not None:\n",
        "        output.append(\"\\\"{}\\\"\".format(text))\n",
        "        output.append(\n",
        "            \"Classified as: {}\".format(model.predict([text]))\n",
        "        )\n",
        "        output.append(\"\")\n",
        "\n",
        "    # Create two columns with most negative and most positive features.\n",
        "    for (cp, fnp), (cn, fnn) in topn:\n",
        "        output.append(\n",
        "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(\n",
        "                cp, fnp, cn, fnn\n",
        "            )\n",
        "        )\n",
        "    #return \"\\n\".join(output)\n",
        "    print(output)\n",
        "\n",
        "show_most_informative_features(logR_pipeline_ngram,vect='LogR_tfidf',clf='LogR_clf')\n",
        "show_most_informative_features(nb_pipeline_ngram,vect='nb_tfidf',clf='nb_clf')\n",
        "show_most_informative_features(svm_pipeline_ngram,vect='svm_tfidf',clf='svm_clf')\n",
        "show_most_informative_features(sgd_pipeline_ngram,vect='sgd_tfidf',clf='sgd_clf')\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}