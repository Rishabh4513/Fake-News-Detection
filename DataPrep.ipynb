{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Nov  4 12:00:49 2017\n",
        "\n",
        "@author: NishitP\n",
        "\"\"\"\n",
        "#import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import seaborn as sb\n",
        "\n",
        "#before reading the files, setup the working directory to point to project repo\n",
        "#reading data files \n",
        "\n",
        "\n",
        "test_filename = 'test.csv'\n",
        "train_filename = 'train.csv'\n",
        "valid_filename = 'valid.csv'\n",
        "\n",
        "train_news = pd.read_csv(train_filename)\n",
        "test_news = pd.read_csv(test_filename)\n",
        "valid_news = pd.read_csv(valid_filename)\n",
        "\n",
        "\n",
        "\n",
        "#data observation\n",
        "def data_obs():\n",
        "    print(\"training dataset size:\")\n",
        "    print(train_news.shape)\n",
        "    print(train_news.head(10))\n",
        "\n",
        "    #below dataset were used for testing and validation purposes\n",
        "    print(test_news.shape)\n",
        "    print(test_news.head(10))\n",
        "    \n",
        "    print(valid_news.shape)\n",
        "    print(valid_news.head(10))\n",
        "\n",
        "#check the data by calling below function\n",
        "#data_obs()\n",
        "\n",
        "#distribution of classes for prediction\n",
        "def create_distribution(dataFile):\n",
        "    \n",
        "    return sb.countplot(x='Label', data=dataFile, palette='hls')\n",
        "    \n",
        "\n",
        "#by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes\n",
        "create_distribution(train_news)\n",
        "create_distribution(test_news)\n",
        "create_distribution(valid_news)\n",
        "\n",
        "\n",
        "#data integrity check (missing label values)\n",
        "#none of the datasets contains missing values therefore no cleaning required\n",
        "def data_qualityCheck():\n",
        "    \n",
        "    print(\"Checking data qualitites...\")\n",
        "    train_news.isnull().sum()\n",
        "    train_news.info()\n",
        "        \n",
        "    print(\"check finished.\")\n",
        "\n",
        "    #below datasets were used to \n",
        "    test_news.isnull().sum()\n",
        "    test_news.info()\n",
        "\n",
        "    valid_news.isnull().sum()\n",
        "    valid_news.info()\n",
        "\n",
        "#run the below function call to see the quality check results\n",
        "#data_qualityCheck()\n",
        "\n",
        "\n",
        "\n",
        "#eng_stemmer = SnowballStemmer('english')\n",
        "#stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#Stemming\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for token in tokens:\n",
        "        stemmed.append(stemmer.stem(token))\n",
        "    return stemmed\n",
        "\n",
        "#process the data\n",
        "def process_data(data,exclude_stopword=True,stem=True):\n",
        "    tokens = [w.lower() for w in data]\n",
        "    tokens_stemmed = tokens\n",
        "    tokens_stemmed = stem_tokens(tokens, eng_stemmer)\n",
        "    tokens_stemmed = [w for w in tokens_stemmed if w not in stopwords ]\n",
        "    return tokens_stemmed\n",
        "\n",
        "\n",
        "#creating ngrams\n",
        "#unigram \n",
        "def create_unigram(words):\n",
        "    assert type(words) == list\n",
        "    return words\n",
        "\n",
        "#bigram\n",
        "def create_bigrams(words):\n",
        "    assert type(words) == list\n",
        "    skip = 0\n",
        "    join_str = \" \"\n",
        "    Len = len(words)\n",
        "    if Len > 1:\n",
        "        lst = []\n",
        "        for i in range(Len-1):\n",
        "            for k in range(1,skip+2):\n",
        "                if i+k < Len:\n",
        "                    lst.append(join_str.join([words[i],words[i+k]]))\n",
        "    else:\n",
        "        #set it as unigram\n",
        "        lst = create_unigram(words)\n",
        "    return lst\n",
        "\n",
        "\"\"\"\n",
        "#trigrams\n",
        "def create_trigrams(words):\n",
        "    assert type(words) == list\n",
        "    skip == 0\n",
        "    join_str = \" \"\n",
        "    Len = len(words)\n",
        "    if L > 2:\n",
        "        lst = []\n",
        "        for i in range(1,skip+2):\n",
        "            for k1 in range(1, skip+2):\n",
        "                for k2 in range(1,skip+2):\n",
        "                    for i+k1 < Len and i+k1+k2 < Len:\n",
        "                        lst.append(join_str.join([words[i], words[i+k1],words[i+k1+k2])])\n",
        "        else:\n",
        "            #set is as bigram\n",
        "            lst = create_bigram(words)\n",
        "    return lst\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "#doc = ['runners like running and thus they run','this is a test for tokens']\n",
        "#tokenizer([word for line in test_news.iloc[:,1] for word in line.lower().split()])\n",
        "\n",
        "#show the distribution of labels in the train and test data\n",
        "\"\"\"def create_datafile(filename)\n",
        "    #function to slice the dataframe to keep variables necessary to be used for classification\n",
        "    return \"return df to be used\"\n",
        "\"\"\"\n",
        "    \n",
        "\"\"\"#converting multiclass labels present in our datasets to binary class labels\n",
        "for i , row in data_TrainNews.iterrows():\n",
        "    if (data_TrainNews.iloc[:,0] == \"mostly-true\" | data_TrainNews.iloc[:,0] == \"half-true\" | data_TrainNews.iloc[:,0] == \"true\"):\n",
        "        data_TrainNews.iloc[:,0] = \"true\"\n",
        "    else :\n",
        "        data_TrainNews.iloc[:,0] = \"false\"\n",
        "        \n",
        "for i,row in data_TrainNews.iterrows():\n",
        "    print(row)\n",
        "\"\"\"\n",
        "    \n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}