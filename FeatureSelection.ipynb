{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Nov  4 14:13:38 2017\n",
        "\n",
        "@author: NishitP\n",
        "\n",
        "Note: before we can train an algorithm to classify fake news labels, we need to extract features from it. It means reducing the mass\n",
        "of unstructured data into some uniform set of attributes that an algorithm can understand. For fake news detection, it could be \n",
        "word counts (bag of words). \n",
        "\"\"\"\n",
        "import DataPrep\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import nltk.corpus \n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "\n",
        "#we will start with simple bag of words technique \n",
        "#creating feature vector - document term matrix\n",
        "countV = CountVectorizer()\n",
        "train_count = countV.fit_transform(DataPrep.train_news['Statement'].values)\n",
        "\n",
        "print(countV)\n",
        "print(train_count)\n",
        "\n",
        "#print training doc term matrix\n",
        "#we have matrix of size of (10240, 12196) by calling below\n",
        "def get_countVectorizer_stats():\n",
        "    \n",
        "    #vocab size\n",
        "    train_count.shape\n",
        "\n",
        "    #check vocabulary using below command\n",
        "    print(countV.vocabulary_)\n",
        "\n",
        "    #get feature names\n",
        "    print(countV.get_feature_names()[:25])\n",
        "\n",
        "\n",
        "#create tf-df frequency features\n",
        "#tf-idf \n",
        "tfidfV = TfidfTransformer()\n",
        "train_tfidf = tfidfV.fit_transform(train_count)\n",
        "\n",
        "def get_tfidf_stats():\n",
        "    train_tfidf.shape\n",
        "    #get train data feature names \n",
        "    print(train_tfidf.A[:10])\n",
        "\n",
        "\n",
        "#bag of words - with n-grams\n",
        "#countV_ngram = CountVectorizer(ngram_range=(1,3),stop_words='english')\n",
        "#tfidf_ngram  = TfidfTransformer(use_idf=True,smooth_idf=True)\n",
        "\n",
        "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)\n",
        "\n",
        "\n",
        "#POS Tagging\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "cutoff = int(.75 * len(tagged_sentences))\n",
        "training_sentences = DataPrep.train_news['Statement']\n",
        " \n",
        "print(training_sentences)\n",
        "\n",
        "#training POS tagger based on words\n",
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }\n",
        "    \n",
        "    \n",
        "#helper function to strip tags from tagged corpus\t\n",
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]\n",
        "\n",
        "\n",
        "\n",
        "#Using Word2Vec \n",
        "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
        "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
        "           for line in lines}\n",
        "\n",
        "\n",
        "\n",
        "#model = gensim.models.Word2Vec(X, size=100) # x be tokenized text\n",
        "#w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
        "\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(word2vec.itervalues().next())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = len(word2vec.itervalues().next())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        # if a word was never seen - it must be at least as infrequent\n",
        "        # as any of the known words - so the default idf is the max of \n",
        "        # known idf's\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])\n",
        "\n",
        "\"\"\"\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}